import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
import json
import re
from datetime import datetime, timedelta
import time
from textblob import TextBlob
from googleapiclient.discovery import build
from newsapi import NewsApiClient
from pytube import YouTube
from youtube_transcript_api import YouTubeTranscriptApi
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
from snownlp import SnowNLP
import jieba



# é é¢è¨­å®š
st.set_page_config(page_title="æ–°èæƒ…æ„Ÿåˆ†æç³»çµ±", page_icon="ğŸ“°", layout="wide")

# å–å¾—æ–°èçš„å‡½å¼
def get_news(query, from_date, to_date, language='zh', sort_by='publishedAt'):
    try:
        #st.write(f"DEBUG: NewsAPI æŸ¥è©¢åƒæ•¸ from={from_date}, to={to_date}, language={language}, query={query}")
        # åˆå§‹åŒ– NewsAPI
        newsapi = NewsApiClient(api_key=news_api_key)
        
        # æŠ“å–æ–°è
        all_articles = newsapi.get_everything(
            q=query,
            from_param=from_date,
            to=to_date,
            language=language,
            sort_by=sort_by
        )
        
        # æ•´ç†è³‡æ–™
        articles = []
        for article in all_articles['articles']:
            #if articles:
                #st.write("DEBUG: å–å¾—æ–°èç™¼ä½ˆæ™‚é–“åˆ†å¸ƒï¼š")
                #st.write(pd.Series([a['ç™¼ä½ˆæ™‚é–“'] for a in articles]).value_counts())
            articles.append({
                'æ¨™é¡Œ': article['title'],
                'ä¾†æº': article['source']['name'],
                'ä½œè€…': article['author'],
                'ç™¼ä½ˆæ™‚é–“': article['publishedAt'],
                'é€£çµ': article['url'],
                'å…§å®¹': article['content'],
                'æè¿°': article['description']
            })
        
        return pd.DataFrame(articles)
    except Exception as e:
        st.error(f"æ“·å–æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return pd.DataFrame()

# æœå°‹ YouTube å½±ç‰‡çš„å‡½å¼
def search_youtube_videos(query, language='zh', max_results=10):
    try:
        # æ ¹æ“šèªè¨€è‡ªå‹•èª¿æ•´æŸ¥è©¢é—œéµå­—
        if language == 'zh':
            search_query = f"{query} ä¸­æ–‡"
        else:
            search_query = f"{query} english"
        youtube = build('youtube', 'v3', developerKey=youtube_api_key)
        search_response = youtube.search().list(
            q=search_query,
            part='id,snippet',
            maxResults=max_results,
            type='video'
        ).execute()
        videos = []
        for item in search_response['items']:
            video_id = item['id'].get('videoId')
            if not video_id:
                continue
            videos.append({
                'æ¨™é¡Œ': item['snippet']['title'],
                'é »é“': item['snippet']['channelTitle'],
                'ç™¼ä½ˆæ™‚é–“': item['snippet']['publishedAt'],
                'å½±ç‰‡ID': video_id,
                'é€£çµ': f'https://www.youtube.com/watch?v={video_id}'
            })
        return pd.DataFrame(videos)
    except Exception as e:
        st.error(f"æœå°‹ YouTube å½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return pd.DataFrame()

# å–å¾— YouTube å½±ç‰‡å­—å¹•çš„å‡½å¼
def get_youtube_transcript(video_id, languages=['zh', 'en']):
    try:
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=languages)
        transcript_text = ' '.join([t['text'] for t in transcript_list])
        return transcript_text
    except Exception as e:
        st.warning(f"ç„¡æ³•æ“·å–æ­¤å½±ç‰‡çš„å­—å¹•ï¼š{e}")
        return ""
# å–å¾— YouTube å½±ç‰‡ç•™è¨€çš„å‡½å¼
def get_youtube_comments(video_id, api_key, max_results=100):
    youtube = build('youtube', 'v3', developerKey=api_key)
    comments = []
    try:
        response = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100 if max_results > 100 else max_results,
            textFormat="plainText"
        ).execute()
        count = 0
        while response and count < max_results:
            for item in response['items']:
                snippet = item['snippet']['topLevelComment']['snippet']
                comments.append({
                    'ç•™è¨€å…§å®¹': snippet['textDisplay'],
                    'ç•™è¨€æ™‚é–“': snippet['publishedAt'],
                    'å½±ç‰‡ID': video_id
                })
                count += 1
                if count >= max_results:
                    break
            if 'nextPageToken' in response and count < max_results:
                response = youtube.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    maxResults=100 if max_results - count > 100 else max_results - count,
                    pageToken=response['nextPageToken'],
                    textFormat="plainText"
                ).execute()
            else:
                break
    except Exception as e:
        st.warning(f"ç„¡æ³•å–å¾—è©•è«–ï¼š{e}")
    return comments

# ä½¿ç”¨ TextBlob é€²è¡Œæƒ…æ„Ÿåˆ†æçš„å‡½å¼
def analyze_sentiment(text):
    if not text or pd.isna(text):
        return {'polarity': 0, 'subjectivity': 0, 'sentiment': 'ä¸­ç«‹'}
    
    # ç°¡å–®ç”¨æ­£å‰‡åˆ¤æ–·ï¼Œå¦‚æœã€Œæ–‡å­—ä¸­å«æœ‰ä¸­æ–‡ã€å°±ç”¨ SnowNLPï¼Œå¦å‰‡ç”¨ TextBlob
    if re.search(r'[\u4e00-\u9fff]', text):
        # ä»¥ SnowNLP è™•ç†ä¸­æ–‡ï¼›SnowNLP çš„ sentiment ä»‹æ–¼ [0,1]ï¼Œ>0.5 è¦–ç‚ºæ­£é¢ï¼Œ<0.5 è¦–ç‚ºè² é¢
        s = SnowNLP(text)
        polarity = (s.sentiments - 0.5) * 2  # è½‰æ›æˆ [-1,1] çš„å°ºåº¦ï¼Œæ–¹ä¾¿ç•«åœ–æˆ–æ¯”è¼ƒ
        subjectivity = None  # SnowNLP æ²’æœ‰æ˜ç¢ºä¸»è§€åº¦æŒ‡æ¨™ï¼Œå…ˆè¨­ç‚º None
        if s.sentiments > 0.5:
            sentiment = 'æ­£é¢'
        elif s.sentiments < 0.5:
            sentiment = 'è² é¢'
        else:
            sentiment = 'ä¸­ç«‹'
    else:
        # å¦‚æœæ²’æœ‰ä¸­æ–‡ï¼Œå°±ä½¿ç”¨ TextBlob åˆ†æï¼ˆè™•ç†è‹±æ–‡ï¼‰
        tb = TextBlob(text)
        polarity = tb.sentiment.polarity
        subjectivity = tb.sentiment.subjectivity
        if polarity > 0:
            sentiment = 'æ­£é¢'
        elif polarity < 0:
            sentiment = 'è² é¢'
        else:
            sentiment = 'ä¸­ç«‹'
    
    return {
        'polarity': polarity,
        'subjectivity': subjectivity,
        'sentiment': sentiment
    }

# è¦–è¦ºåŒ–æƒ…æ„Ÿåˆ†ä½ˆçš„å‡½å¼
def visualize_sentiment(df, language):
    if df.empty:
        st.warning("æ²’æœ‰å¯è¦–è¦ºåŒ–çš„è³‡æ–™")
        return

    # ----- 1. ç¹ªè£½æƒ…æ„Ÿåˆ†ä½ˆåœ“é¤…åœ– -----
    sentiment_counts = df['sentiment'].value_counts().reset_index()
    sentiment_counts.columns = ['æƒ…æ„Ÿ', 'æ•¸é‡']
    fig1 = px.pie(
        sentiment_counts,
        values='æ•¸é‡',
        names='æƒ…æ„Ÿ',
        title='æƒ…æ„Ÿåˆ†ä½ˆ',
        color='æƒ…æ„Ÿ',
        color_discrete_map={'æ­£é¢': 'green', 'ä¸­ç«‹': 'blue', 'è² é¢': 'red'}
    )
    st.plotly_chart(fig1)

    # ----- 2. ç¹ªè£½ä¸åŒä¾†æºä¹‹æƒ…æ„Ÿé•·æ¢åœ– -----
    if 'ä¾†æº' in df.columns:
        top_sources = df.groupby(['ä¾†æº', 'sentiment']).size().reset_index(name='count')
        fig2 = px.bar(
            top_sources,
            x='ä¾†æº',
            y='count',
            color='sentiment',
            title='ä¸åŒä¾†æºä¹‹æƒ…æ„Ÿåˆ†æ',
            color_discrete_map={'æ­£é¢': 'green', 'ä¸­ç«‹': 'blue', 'è² é¢': 'red'}
        )
        st.plotly_chart(fig2)

    # ----- 3. ç¹ªè£½æ™‚é–“åºåˆ—è¶¨å‹¢åœ–ï¼šä¾æ—¥æœŸçœ‹æ­£ã€ä¸­ã€è² çš„è®ŠåŒ– -----
    #st.write("DEBUG: DataFrame columns:", df.columns.tolist())
    if 'ç™¼ä½ˆæ™‚é–“' in df.columns:
        #st.write("DEBUG: Sample 'ç™¼ä½ˆæ™‚é–“' values:", df['ç™¼ä½ˆæ™‚é–“'].head().tolist())
        try:
            # ...existing code...
            df['æ—¥æœŸ'] = pd.to_datetime(df['ç™¼ä½ˆæ™‚é–“'], errors='coerce').dt.date
            df = df[~df['æ—¥æœŸ'].isna()]
            df['sentiment'] = df['sentiment'].astype(str).str.strip()
            df = df[df['sentiment'].isin(['æ­£é¢', 'ä¸­ç«‹', 'è² é¢'])]

            # å…ˆ groupby
            date_sentiment = df.groupby(['æ—¥æœŸ', 'sentiment']).size().reset_index(name='count')

            # å†é¡¯ç¤º
            #st.dataframe(df[['æ—¥æœŸ', 'sentiment']].head(20))
            #st.dataframe(df['sentiment'].value_counts())
            #st.dataframe(date_sentiment)

            #st.write("DEBUG: date_sentiment shape:", date_sentiment.shape)
            #st.write("DEBUG: date_sentiment head:", date_sentiment.head(10))

            #st.write("DEBUG: éæ¿¾å¾Œè³‡æ–™ç­†æ•¸ï¼š", len(df))
            #st.write("DEBUG: æ—¥æœŸæ¬„ NaN æ•¸é‡ï¼š", df['æ—¥æœŸ'].isna().sum())
            #st.write("DEBUG: Sample 'sentiment' values:", df['sentiment'].head(10).tolist())
            #st.write("DEBUG: sentiment dtype:", df['sentiment'].dtype)
            #st.write("DEBUG: sentiment unique:", df['sentiment'].unique())
            #st.write("DEBUG: sentiment value counts:", df['sentiment'].value_counts())

            if not date_sentiment.empty:
                fig3 = px.line(
                    date_sentiment,
                    x='æ—¥æœŸ',
                    y='count',
                    color='sentiment',
                    title='æƒ…æ„Ÿè¶¨å‹¢ï¼ˆä¾æ—¥æœŸï¼‰',
                    color_discrete_map={'æ­£é¢': 'green', 'ä¸­ç«‹': 'blue', 'è² é¢': 'red'}
                )
                st.plotly_chart(fig3)
            else:
                st.info("DEBUG: æ²’æœ‰æƒ…æ„Ÿè¶¨å‹¢è³‡æ–™å¯ä¾›ç¹ªè£½ã€‚")
            # ...existing code...
        except Exception as e:
            st.error(f"æ—¥æœŸè½‰æ›æˆ–ç¹ªåœ–æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    else:
        st.info("DEBUG: DataFrame ä¸­æ²’æœ‰ 'ç™¼ä½ˆæ™‚é–“' æ¬„ã€‚")

    # ----- 4. ç”¢ç”Ÿè©é›² -----
    # 4.1 æº–å‚™ text_corpus
    # å½±ç‰‡è©é›²
    text_corpus = ""
    if language == 'zh':
        parts = []
        if 'æ¨™é¡Œ' in df.columns:
            parts.append(" ".join(df['æ¨™é¡Œ'].dropna().astype(str).tolist()))
        if 'æè¿°' in df.columns:
            parts.append(" ".join(df['æè¿°'].dropna().astype(str).tolist()))
        if 'å…§å®¹' in df.columns:
            parts.append(" ".join(df['å…§å®¹'].dropna().astype(str).tolist()))
        # æ–°å¢ç•™è¨€æ‘˜è¦
        if 'ç•™è¨€æ‘˜è¦' in df.columns:
            parts.append(" ".join(df['ç•™è¨€æ‘˜è¦'].dropna().astype(str).tolist()))
        raw = " ".join(parts)
        chinese_text = "".join(re.findall(r'[\u4e00-\u9fff]+', raw))
        text_corpus = chinese_text
    else:  # language == 'en'
        parts = []
        if 'æè¿°' in df.columns:
            parts.append(" ".join(df['æè¿°'].dropna().astype(str).tolist()))
        if 'å…§å®¹' in df.columns:
            parts.append(" ".join(df['å…§å®¹'].dropna().astype(str).tolist()))
        # æ–°å¢ç•™è¨€æ‘˜è¦
        if 'ç•™è¨€æ‘˜è¦' in df.columns:
            parts.append(" ".join(df['ç•™è¨€æ‘˜è¦'].dropna().astype(str).tolist()))
        text_corpus = " ".join(parts)
    # å½±ç‰‡è©é›²çµæŸ.

    # æ–°èè©é›².
    text_corpus = ""
    if language == 'zh':
        # æŠŠ æ¨™é¡Œã€æè¿°ã€å…§å®¹ è£¡çš„ä¸­æ–‡éƒ½ä¸²èµ·ä¾†
        parts = []
        if 'æ¨™é¡Œ' in df.columns:
            parts.append(" ".join(df['æ¨™é¡Œ'].dropna().astype(str).tolist()))
        if 'æè¿°' in df.columns:
            parts.append(" ".join(df['æè¿°'].dropna().astype(str).tolist()))
        if 'å…§å®¹' in df.columns:
            parts.append(" ".join(df['å…§å®¹'].dropna().astype(str).tolist()))
        raw = " ".join(parts)

        # åªç•™ä¸‹æ‰€æœ‰é€£çºŒçš„æ¼¢å­—
        chinese_text = "".join(re.findall(r'[\u4e00-\u9fff]+', raw))
        text_corpus = chinese_text
    else:  # language == 'en'
        # è‹±æ–‡å°±æŠ“æè¿°+å…§å®¹ï¼Œç›´æ¥æ‹¼èµ·ä¾†
        parts = []
        if 'æè¿°' in df.columns:
            parts.append(" ".join(df['æè¿°'].dropna().astype(str).tolist()))
        if 'å…§å®¹' in df.columns:
            parts.append(" ".join(df['å…§å®¹'].dropna().astype(str).tolist()))
        text_corpus = " ".join(parts)

    # 4.2 å¦‚æœæœ‰ text_corpusï¼Œç¹ªè£½è©é›²
    if text_corpus:
        st.subheader("æ–‡å­—è©é›²å±•ç¤º")

        if language == 'zh':
            # åªå‰©ä¸­æ–‡æ‰ç”¨ jieba åˆ†è©
            tokens = " ".join(jieba.cut(text_corpus))
            wc = WordCloud(
                font_path="/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
                width=800,
                height=400,
                background_color='white',
                max_words=100,
                stopwords=None,       # å¦‚è¦åŠ ä¸­æ–‡åœç”¨è©ï¼Œåœ¨æ­¤å‚³å…¥é›†åˆ
                collocations=False
            ).generate(tokens)
        else:
            # è‹±æ–‡ç›´æ¥ç”¨åŸå§‹æ–‡å­—
            wc = WordCloud(
                width=800,
                height=400,
                background_color='white',
                max_words=100,
                stopwords=None,       # å¦‚è¦åŠ è‹±æ–‡åœç”¨è©ï¼Œå¯å‚³ EN_STOPWORDS
                collocations=False
            ).generate(text_corpus)

        fig_wc, ax_wc = plt.subplots(figsize=(10, 5))
        ax_wc.imshow(wc, interpolation='bilinear')
        ax_wc.axis('off')
        st.pyplot(fig_wc)
    else:
        st.info("ç„¡å¯ç”¨æ–‡å­—ç”Ÿæˆè©é›²ã€‚")
    # æ–°èè©é›²çµæŸ.


# é¡¯ç¤ºåˆ†æçµæœçš„å‡½å¼
def display_results(df, source_type):
    if df.empty:
        st.warning(f"æ‰¾ä¸åˆ°ä»»ä½• {source_type}")
        return
    
    # é¡¯ç¤ºåŸºæœ¬è³‡è¨Š
    st.subheader(f"{source_type} åˆ†æçµæœ")
    st.write(f"å…±åˆ†æ {len(df)} å€‹ {source_type}")
    
    # é¡¯ç¤ºæƒ…æ„Ÿåˆ†ä½ˆ
    sentiment_distribution = df['sentiment'].value_counts()
    st.write("æƒ…æ„Ÿåˆ†ä½ˆï¼š")
    st.write(sentiment_distribution)
    
    # é¡¯ç¤ºè³‡æ–™è¡¨
    st.subheader(f"{source_type} è³‡æ–™è¡¨")
    st.dataframe(df)
    
    # æƒ…æ„Ÿè¦–è¦ºåŒ–
    st.subheader(f"{source_type} æƒ…æ„Ÿè¦–è¦ºåŒ–")
    visualize_sentiment(df, language)
    
    # é¡¯ç¤ºæƒ…æ„Ÿæœ€æ¥µç«¯çš„é …ç›®
    st.subheader(f"{source_type} ä¸­æƒ…æ„Ÿæœ€æ­£å‘çš„é …ç›®")
    most_positive = df.loc[df['polarity'].idxmax()]
    st.write(f"æ¨™é¡Œï¼š{most_positive.get('æ¨™é¡Œ', 'ç„¡')}")
    st.write(f"æ¥µæ€§æŒ‡æ•¸ï¼š{most_positive['polarity']:.4f}")
    st.write(f"é€£çµï¼š{most_positive.get('é€£çµ', 'ç„¡')}")
    
    st.subheader(f"{source_type} ä¸­æƒ…æ„Ÿæœ€è² å‘çš„é …ç›®")
    most_negative = df.loc[df['polarity'].idxmin()]
    st.write(f"æ¨™é¡Œï¼š{most_negative.get('æ¨™é¡Œ', 'ç„¡')}")
    st.write(f"æ¥µæ€§æŒ‡æ•¸ï¼š{most_negative['polarity']:.4f}")
    st.write(f"é€£çµï¼š{most_negative.get('é€£çµ', 'ç„¡')}")

# ä¸»ä»‹é¢
st.title("ğŸ“° æ–°èæƒ…æ„Ÿåˆ†æç³»çµ±")
st.markdown("""
æœ¬ç³»çµ±å¯æœå°‹ä¸¦åˆ†æç¶²è·¯ä¸Šçš„æ–°èå…§å®¹ï¼ˆåŒ…å«æ–‡å­—èˆ‡å½±ç‰‡ï¼‰ï¼Œé¡¯ç¤ºå…¶æƒ…æ„Ÿå‚¾å‘ã€‚
""")

# å´é‚Šæ¬„ï¼šAPI è¨­å®šèˆ‡æœå°‹åƒæ•¸
with st.sidebar:
    st.header("API è¨­å®š")
    llm_api_key = st.text_input("LLM API é‡‘é‘°", type="password")  # æ–°å¢é€™ä¸€è¡Œ
    news_api_key = st.text_input("NewsAPI é‡‘é‘°", type="password")
    youtube_api_key = st.text_input("YouTube API é‡‘é‘°", type="password")
    
    st.header("æœå°‹è¨­å®š")
    query = st.text_input("æœå°‹é—œéµå­—ï¼ˆä¸­è‹±æ–‡çš†å¯ï¼‰")
    
    col1, col2 = st.columns(2)
    with col1:
        days_ago = st.number_input("æœå°‹è‡³å¹¾å¤©å‰çš„æ–°è", min_value=1, max_value=30, value=7)
    with col2:
        language = st.selectbox("èªè¨€", options=['zh', 'en'], index=0)
    
    max_results = st.slider("æœ€å¤šé¡¯ç¤ºå½±ç‰‡æ•¸é‡", min_value=5, max_value=50, value=10)
    
    search_type = st.multiselect("æœå°‹é¡å‹", ['æ–‡å­—æ–°è', 'YouTube å½±ç‰‡'], default=['æ–‡å­—æ–°è'])
    
    analyze_button = st.button("é–‹å§‹åˆ†æ")

# é¡¯ç¤ºçµæœå€åŸŸ
if analyze_button and query:
    all_results = {}
    if not news_api_key and 'æ–‡å­—æ–°è' in search_type:
        st.error("è«‹è¼¸å…¥ NewsAPI é‡‘é‘°ä»¥æœå°‹æ–‡å­—æ–°è")
    
    if not youtube_api_key and 'YouTube å½±ç‰‡' in search_type:
        st.error("è«‹è¼¸å…¥ YouTube API é‡‘é‘°ä»¥æœå°‹å½±ç‰‡")
    
    if (news_api_key and 'æ–‡å­—æ–°è' in search_type) or (youtube_api_key and 'YouTube å½±ç‰‡' in search_type):
        with st.spinner("æ­£åœ¨æœå°‹èˆ‡åˆ†æï¼Œè«‹ç¨å€™..."):
            # æ—¥æœŸç¯„åœ
            to_date = datetime.now().strftime('%Y-%m-%d')
            from_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')
            
            # å»ºç«‹åˆ†é 
            tabs = st.tabs([t for t in search_type] + ["ç¸½çµ", "AI åŠ©ç†"])
        
            
            # è™•ç†æ–‡å­—æ–°è
            if 'æ–‡å­—æ–°è' in search_type and news_api_key:
                with tabs[search_type.index('æ–‡å­—æ–°è')]:
                    st.header("æ–‡å­—æ–°èæƒ…æ„Ÿåˆ†æ")
                    
                    news_df = get_news(query, from_date, to_date, language)
                    
                    if not news_df.empty:
                        sentiment_results = []
                        for _, row in news_df.iterrows():
                            text_to_analyze = f"{row['æ¨™é¡Œ']} {row['æè¿°']} {row['å…§å®¹']}"
                            sentiment_data = analyze_sentiment(text_to_analyze)
                            sentiment_results.append(sentiment_data)
                        
                        news_df['polarity'] = [r['polarity'] for r in sentiment_results]
                        news_df['subjectivity'] = [r['subjectivity'] for r in sentiment_results]
                        news_df['sentiment'] = [r['sentiment'] for r in sentiment_results]
                        
                        display_results(news_df, "æ–‡å­—æ–°è")
                        all_results['æ–‡å­—æ–°è'] = news_df
                    else:
                        st.warning("æ‰¾ä¸åˆ°ä»»ä½•æ–°èè³‡æ–™ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—æˆ–å»¶é•·æ™‚é–“ç¯„åœã€‚")
            
            # è™•ç† YouTube å½±ç‰‡
            if 'YouTube å½±ç‰‡' in search_type and youtube_api_key:
                with tabs[search_type.index('YouTube å½±ç‰‡')]:
                    st.header("YouTube å½±ç‰‡ç•™è¨€æƒ…æ„Ÿåˆ†æ")
                    st.info("âš ï¸ æœ¬åŠŸèƒ½åˆ†æçš„æ˜¯å½±ç‰‡ä¸‹æ–¹çš„ç•™è¨€è©•è«–ï¼Œä¸æ˜¯å­—å¹•ã€‚")

                    videos_df = search_youtube_videos(query, language, max_results)

                    if not videos_df.empty:
                        progress_bar = st.progress(0)
                        progress_text = st.empty()



                        all_comments = []
                        for i, (_, row) in enumerate(videos_df.iterrows()):
                            progress_text.text(f"æ­£åœ¨åˆ†æç¬¬ {i+1} éƒ¨å½±ç‰‡ï¼Œå…± {len(videos_df)} éƒ¨")
                            comments = get_youtube_comments(row['å½±ç‰‡ID'], youtube_api_key, max_results=100)
                            for c in comments:
                                c['å½±ç‰‡æ¨™é¡Œ'] = row['æ¨™é¡Œ']
                                c['å½±ç‰‡ç™¼ä½ˆæ™‚é–“'] = row['ç™¼ä½ˆæ™‚é–“']
                            all_comments.extend(comments)
                            progress_bar.progress((i + 1) / len(videos_df))

                        progress_bar.empty()
                        progress_text.empty()

                        if all_comments:
                            comments_df = pd.DataFrame(all_comments)
                            # å°æ¯å‰‡ç•™è¨€åšæƒ…æ„Ÿåˆ†æ
                            sentiment_results = [analyze_sentiment(c) for c in comments_df['ç•™è¨€å…§å®¹']]
                            comments_df['polarity'] = [r['polarity'] for r in sentiment_results]
                            comments_df['subjectivity'] = [r['subjectivity'] for r in sentiment_results]
                            comments_df['sentiment'] = [r['sentiment'] for r in sentiment_results]
                            comments_df.rename(columns={'ç•™è¨€å…§å®¹': 'å…§å®¹', 'ç•™è¨€æ™‚é–“': 'ç™¼ä½ˆæ™‚é–“'}, inplace=True)
                            display_results(comments_df, "YouTube ç•™è¨€")
                            all_results['YouTube å½±ç‰‡'] = comments_df
                        else:
                            st.warning("æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€è³‡æ–™ã€‚")
                        # <<< é€™è£¡çµæŸ >>>
            
            # ç¸½çµåˆ†é 
            with tabs[-2]:
                st.header("æƒ…æ„Ÿåˆ†æç¸½çµ")
                
                if all_results:
                    summary_data = []
                    for source_type, df in all_results.items():
                        if not df.empty:
                            source_summary = {
                                'é¡å‹': source_type,
                                'ç¸½æ•¸': len(df),
                                'æ­£é¢': sum(df['sentiment'] == 'æ­£é¢'),
                                'ä¸­ç«‹': sum(df['sentiment'] == 'ä¸­ç«‹'),
                                'è² é¢': sum(df['sentiment'] == 'è² é¢'),
                                'å¹³å‡æ¥µæ€§': df['polarity'].mean(),
                                'å¹³å‡ä¸»è§€æ€§': df['subjectivity'].mean()
                            }
                            summary_data.append(source_summary)
                    
                    if summary_data:
                        summary_df = pd.DataFrame(summary_data)
                        st.write("ç¸½çµè³‡æ–™ï¼š")
                        st.dataframe(summary_df)
                        
                        # ç¹ªè£½ç¸½çµé•·æ¢åœ–
                        fig = go.Figure()
                        for source_type in summary_df['é¡å‹']:
                            row = summary_df[summary_df['é¡å‹'] == source_type].iloc[0]
                            fig.add_trace(go.Bar(
                                name=source_type,
                                x=['æ­£é¢', 'ä¸­ç«‹', 'è² é¢'],
                                y=[row['æ­£é¢'], row['ä¸­ç«‹'], row['è² é¢']],
                                marker_color=['green', 'blue', 'red']
                            ))
                        
                        fig.update_layout(
                            title='ä¸åŒä¾†æºä¹‹æƒ…æ„Ÿåˆ†ä½ˆ',
                            xaxis_title='æƒ…æ„Ÿé¡åˆ¥',
                            yaxis_title='æ•¸é‡',
                            barmode='group'
                        )
                        st.plotly_chart(fig)
                        
                        # æœ€çµ‚çµè«–
                        st.subheader("æœ€çµ‚çµè«–")
                        
                        total_positive = sum(row['æ­£é¢'] for row in summary_data)
                        total_neutral = sum(row['ä¸­ç«‹'] for row in summary_data)
                        total_negative = sum(row['è² é¢'] for row in summary_data)
                        total_items = sum(row['ç¸½æ•¸'] for row in summary_data)
                        
                        st.write(f"ç¸½å…±åˆ†æ {total_items} ç­†è³‡æ–™ï¼š")
                        st.write(f"- {total_positive} ({total_positive/total_items*100:.1f}%) ç‚ºæ­£é¢")
                        st.write(f"- {total_neutral} ({total_neutral/total_items*100:.1f}%) ç‚ºä¸­ç«‹")
                        st.write(f"- {total_negative} ({total_negative/total_items*100:.1f}%) ç‚ºè² é¢")
                        
                        if total_positive > (total_neutral + total_negative):
                            st.write(f"**çµè«–ï¼š** é—œéµå­—ã€Œ{query}ã€æ•´é«”å‘ˆç¾**æ­£é¢**æƒ…æ„Ÿå‚¾å‘ã€‚")
                        elif total_negative > (total_neutral + total_positive):
                            st.write(f"**çµè«–ï¼š** é—œéµå­—ã€Œ{query}ã€æ•´é«”å‘ˆç¾**è² é¢**æƒ…æ„Ÿå‚¾å‘ã€‚")
                        else:
                            st.write(f"**çµè«–ï¼š** é—œéµå­—ã€Œ{query}ã€æ•´é«”æƒ…æ„Ÿå‚¾å‘**ä¸­ç«‹æˆ–æ··åˆ**ã€‚")
                else:
                    st.warning("ç›®å‰æ²’æœ‰ä»»ä½•è³‡æ–™å¯ä¾›ç¸½çµåˆ†æã€‚")
            # AI åŠ©ç†åˆ†é 
            with tabs[-1]:
                st.header("AI åŠ©ç†")
                st.markdown("ä½ å¯ä»¥è©¢å•æœ¬æ¬¡åˆ†æçš„ä»»ä½•å•é¡Œï¼ŒAI æœƒæ ¹æ“šåˆ†æçµæœå›è¦†ã€‚")
                user_question = st.text_area("è«‹è¼¸å…¥ä½ çš„å•é¡Œ", value="é€™æ¬¡çš„é—œéµè©æƒ…æ„Ÿåˆ†æçµæœä½ æœ‰ç”šéº¼çœ‹æ³•ï¼Ÿ")
                ask_button = st.button("é€å‡ºå•é¡Œ", key="ask_llm")

                if "llm_reply" not in st.session_state:
                    st.session_state.llm_reply = ""

                if ask_button and user_question and llm_api_key:
                    # å½™æ•´æ‰€æœ‰åˆ†æçµæœ
                    summary_text = ""
                    for source_type, df in all_results.items():
                        if not df.empty:
                            summary_text += f"\nã€{source_type}ã€‘\n"
                            summary_text += df.head(10).to_markdown(index=False)
                    prompt = f"ä»¥ä¸‹æ˜¯æœ¬æ¬¡æƒ…æ„Ÿåˆ†æçš„è³‡æ–™æ‘˜è¦ï¼š\n{summary_text}\n\nä½¿ç”¨è€…å•é¡Œï¼š{user_question}\nè«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡è¦å›è¦†ã€‚"

                    headers = {
                        "Authorization": f"Bearer {llm_api_key}",
                        "Content-Type": "application/json"
                    }
                    data = {
                        "model": "gpt-3.5-turbo",
                        "messages": [
                            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æ–°èæƒ…æ„Ÿåˆ†æåŠ©ç†ï¼Œè«‹æ ¹æ“šè³‡æ–™æ‘˜è¦å›ç­”å•é¡Œã€‚"},
                            {"role": "user", "content": prompt}
                        ]
                    }
                    try:
                        response = requests.post(
                            "https://api.openai.com/v1/chat/completions",
                            headers=headers,
                            json=data,
                            timeout=60
                        )
                        if response.status_code == 200:
                            ai_reply = response.json()["choices"][0]["message"]["content"]
                            st.session_state.llm_reply = ai_reply
                        else:
                            st.session_state.llm_reply = f"AI å›æ‡‰å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}\n{response.text}"
                    except Exception as e:
                        st.session_state.llm_reply = f"AI åŠ©ç†å‘¼å«å¤±æ•—ï¼š{e}"

                elif ask_button and not llm_api_key:
                    st.warning("è«‹å…ˆè¼¸å…¥ LLM API é‡‘é‘°")

                # é¡¯ç¤º LLM å›æ‡‰
                if st.session_state.llm_reply:
                    st.success("AI åŠ©ç†å›è¦†ï¼š")
                    st.write(st.session_state.llm_reply)
        
        # æä¾›ä¸‹è¼‰åŠŸèƒ½
        if all_results:
            st.subheader("ä¸‹è¼‰åˆ†æçµæœ")
            for source_type, df in all_results.items():
                if not df.empty:
                    csv = df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label=f"ä¸‹è¼‰ {source_type} çµæœ (CSV)",
                        data=csv,
                        file_name=f"åˆ†æçµæœ_{source_type.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.csv",
                        mime="text/csv"
                    )

else:
    st.info("è«‹è¼¸å…¥æœå°‹é—œéµå­—ä¸¦é»æ“Šã€Œé–‹å§‹åˆ†æã€ä»¥å•Ÿå‹•ç³»çµ±ã€‚")
    
    with st.expander("ä½¿ç”¨èªªæ˜"):
        st.markdown("""
        ### å¦‚ä½•ä½¿ç”¨æ­¤ç³»çµ±
        
        1. **API è¨­å®š**  
           - å–å¾— [NewsAPI](https://newsapi.org) çš„ API Keyï¼Œä»¥åŠåœ¨ Google Cloud Console å•Ÿç”¨ YouTube Data API å¾Œå–å¾— API Key  
           - å°‡é‡‘é‘°åˆ†åˆ¥è²¼åˆ°å´é‚Šæ¬„çš„ã€ŒNewsAPI é‡‘é‘°ã€èˆ‡ã€ŒYouTube API é‡‘é‘°ã€æ¬„ä½
        
        2. **æœå°‹è¨­å®š**  
           - è¼¸å…¥æ¬²æœå°‹çš„é—œéµå­—ï¼ˆå¯åŒæ™‚ä½¿ç”¨ä¸­æ–‡èˆ‡è‹±æ–‡ï¼‰  
           - é¸æ“‡è¦æŠ“å–å¤šå°‘å¤©å…§çš„æ–°è  
           - é¸æ“‡èªè¨€ï¼ˆä¸­æ–‡ or è‹±æ–‡ï¼‰  
           - é¸æ“‡è¦åˆ†æçš„é¡å‹ï¼ˆæ–‡å­—æ–°èã€YouTube å½±ç‰‡ï¼Œæˆ–å…©è€…ï¼‰
        
        3. **æŸ¥çœ‹çµæœ**  
           - è§€å¯Ÿæƒ…æ„Ÿåˆ†ä½ˆï¼ˆæ­£é¢ã€ä¸­ç«‹ã€è² é¢ï¼‰  
           - æŸ¥çœ‹ä¸åŒä¾†æºçš„æƒ…æ„Ÿåˆ†æ  
           - æŸ¥çœ‹æƒ…æ„Ÿè¶¨å‹¢åœ–èˆ‡æœ€æ¥µç«¯çš„æ­£/è² å‘é …ç›®
        
        **ç¯„ä¾‹é—œéµå­—ï¼š**  
        - æ”¿æ²» (Politics)  
        - ç¶“æ¿Ÿ (Economy)  
        - ç§‘æŠ€ (Technology)  
        - å¥åº· (Health)  
        - ç’°ä¿ (Environment)  
        """)
        
# é é¢åº•éƒ¨
st.markdown("---")
st.markdown("Â© 2025 æ–°èæƒ…æ„Ÿåˆ†æç³»çµ± | ä½¿ç”¨ Streamlitã€NewsAPI å’Œ YouTube API é–‹ç™¼")
