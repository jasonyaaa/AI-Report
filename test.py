import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
import json
import re
from datetime import datetime, timedelta
import time
from textblob import TextBlob
from googleapiclient.discovery import build
from newsapi import NewsApiClient
from pytube import YouTube
from youtube_transcript_api import YouTubeTranscriptApi
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
from snownlp import SnowNLP
import jieba


# é é¢è¨­å®š
st.set_page_config(page_title="æ–°èæƒ…æ„Ÿåˆ†æAIç³»çµ±", page_icon="ğŸ“°", layout="wide")

# å–å¾—æ–°èçš„å‡½å¼
def get_news(query, from_date, to_date, language='zh', sort_by='publishedAt'):
    try:
        #st.write(f"DEBUG: NewsAPI æŸ¥è©¢åƒæ•¸ from={from_date}, to={to_date}, language={language}, query={query}")
        # åˆå§‹åŒ– NewsAPI
        newsapi = NewsApiClient(api_key=news_api_key)
        
        # æŠ“å–æ–°è
        all_articles = newsapi.get_everything(
            q=query,
            from_param=from_date,
            to=to_date,
            language=language,
            sort_by=sort_by
        )
        
        # æ•´ç†è³‡æ–™
        articles = []
        for article in all_articles['articles']:
            #if articles:
                #st.write("DEBUG: å–å¾—æ–°èç™¼ä½ˆæ™‚é–“åˆ†å¸ƒï¼š")
                #st.write(pd.Series([a['ç™¼ä½ˆæ™‚é–“'] for a in articles]).value_counts())
            articles.append({
                'æ¨™é¡Œ': article['title'],
                'ä¾†æº': article['source']['name'],
                'ä½œè€…': article['author'],
                'ç™¼ä½ˆæ™‚é–“': article['publishedAt'],
                'é€£çµ': article['url'],
                'å…§å®¹': article['content'],
                'æè¿°': article['description']
            })
        
        return pd.DataFrame(articles)
    except Exception as e:
        st.error(f"æ“·å–æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return pd.DataFrame()

# æœå°‹ YouTube å½±ç‰‡çš„å‡½å¼
def search_youtube_videos(query, language='zh', max_results=10):
    try:
        # æ ¹æ“šèªè¨€è‡ªå‹•èª¿æ•´æŸ¥è©¢é—œéµå­—
        if language == 'zh':
            search_query = f"{query} ä¸­æ–‡"
        else:
            search_query = f"{query} english"
        youtube = build('youtube', 'v3', developerKey=youtube_api_key)
        search_response = youtube.search().list(
            q=search_query,
            part='id,snippet',
            maxResults=max_results,
            type='video'
        ).execute()
        videos = []
        for item in search_response['items']:
            video_id = item['id'].get('videoId')
            if not video_id:
                continue
            videos.append({
                'æ¨™é¡Œ': item['snippet']['title'],
                'é »é“': item['snippet']['channelTitle'],
                'ç™¼ä½ˆæ™‚é–“': item['snippet']['publishedAt'],
                'å½±ç‰‡ID': video_id,
                'é€£çµ': f'https://www.youtube.com/watch?v={video_id}'
            })
        return pd.DataFrame(videos)
    except Exception as e:
        st.error(f"æœå°‹ YouTube å½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return pd.DataFrame()

# å–å¾— YouTube å½±ç‰‡å­—å¹•çš„å‡½å¼
def get_youtube_transcript(video_id, languages=['zh', 'en']):
    try:
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=languages)
        transcript_text = ' '.join([t['text'] for t in transcript_list])
        return transcript_text
    except Exception as e:
        st.warning(f"ç„¡æ³•æ“·å–æ­¤å½±ç‰‡çš„å­—å¹•ï¼š{e}")
        return ""
# å–å¾— YouTube å½±ç‰‡ç•™è¨€çš„å‡½å¼
def get_youtube_comments(video_id, api_key, max_results=100):
    youtube = build('youtube', 'v3', developerKey=api_key)
    comments = []
    try:
        response = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100 if max_results > 100 else max_results,
            textFormat="plainText"
        ).execute()
        count = 0
        while response and count < max_results:
            for item in response['items']:
                snippet = item['snippet']['topLevelComment']['snippet']
                comments.append({
                    'ç•™è¨€å…§å®¹': snippet['textDisplay'],
                    'ç•™è¨€æ™‚é–“': snippet['publishedAt'],
                    'å½±ç‰‡ID': video_id
                })
                count += 1
                if count >= max_results:
                    break
            if 'nextPageToken' in response and count < max_results:
                response = youtube.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    maxResults=100 if max_results - count > 100 else max_results - count,
                    pageToken=response['nextPageToken'],
                    textFormat="plainText"
                ).execute()
            else:
                break
    except Exception as e:
        st.warning(f"ç„¡æ³•å–å¾—è©•è«–ï¼š{e}")
    return comments

# ä½¿ç”¨ TextBlob é€²è¡Œæƒ…æ„Ÿåˆ†æçš„å‡½å¼
def analyze_sentiment(text):
    if not text or pd.isna(text):
        return {'polarity': 0, 'subjectivity': 0, 'sentiment': 'ä¸­ç«‹'}
    
    # ç°¡å–®ç”¨æ­£å‰‡åˆ¤æ–·ï¼Œå¦‚æœã€Œæ–‡å­—ä¸­å«æœ‰ä¸­æ–‡ã€å°±ç”¨ SnowNLPï¼Œå¦å‰‡ç”¨ TextBlob
    if re.search(r'[\u4e00-\u9fff]', text):
        # ä»¥ SnowNLP è™•ç†ä¸­æ–‡ï¼›SnowNLP çš„ sentiment ä»‹æ–¼ [0,1]ï¼Œ>0.5 è¦–ç‚ºæ­£é¢ï¼Œ<0.5 è¦–ç‚ºè² é¢
        s = SnowNLP(text)
        polarity = (s.sentiments - 0.5) * 2  # è½‰æ›æˆ [-1,1] çš„å°ºåº¦ï¼Œæ–¹ä¾¿ç•«åœ–æˆ–æ¯”è¼ƒ
        subjectivity = None  # SnowNLP æ²’æœ‰æ˜ç¢ºä¸»è§€åº¦æŒ‡æ¨™ï¼Œå…ˆè¨­ç‚º None
        if s.sentiments > 0.5:
            sentiment = 'æ­£é¢'
        elif s.sentiments < 0.5:
            sentiment = 'è² é¢'
        else:
            sentiment = 'ä¸­ç«‹'
    else:
        # å¦‚æœæ²’æœ‰ä¸­æ–‡ï¼Œå°±ä½¿ç”¨ TextBlob åˆ†æï¼ˆè™•ç†è‹±æ–‡ï¼‰
        tb = TextBlob(text)
        polarity = tb.sentiment.polarity
        subjectivity = tb.sentiment.subjectivity
        if polarity > 0:
            sentiment = 'æ­£é¢'
        elif polarity < 0:
            sentiment = 'è² é¢'
        else:
            sentiment = 'ä¸­ç«‹'
    
    return {
        'polarity': polarity,
        'subjectivity': subjectivity,
        'sentiment': sentiment
    }

# è¦–è¦ºåŒ–æƒ…æ„Ÿåˆ†ä½ˆçš„å‡½å¼
def visualize_sentiment(df, language):
    if df.empty:
        st.warning("æ²’æœ‰å¯è¦–è¦ºåŒ–çš„è³‡æ–™")
        return

    # ----- 1. ç¹ªè£½æƒ…æ„Ÿåˆ†ä½ˆåœ“é¤…åœ– -----
    sentiment_counts = df['sentiment'].value_counts().reset_index()
    sentiment_counts.columns = ['æƒ…æ„Ÿ', 'æ•¸é‡']
    fig1 = px.pie(
        sentiment_counts,
        values='æ•¸é‡',
        names='æƒ…æ„Ÿ',
        title='æƒ…æ„Ÿåˆ†ä½ˆ',
        color='æƒ…æ„Ÿ',
        color_discrete_map={'æ­£é¢': 'green', 'ä¸­ç«‹': 'blue', 'è² é¢': 'red'}
    )
    st.plotly_chart(fig1)

    # ----- 2. ç¹ªè£½ä¸åŒä¾†æºä¹‹æƒ…æ„Ÿé•·æ¢åœ– -----
    if 'ä¾†æº' in df.columns:
        top_sources = df.groupby(['ä¾†æº', 'sentiment']).size().reset_index(name='count')
        fig2 = px.bar(
            top_sources,
            x='ä¾†æº',
            y='count',
            color='sentiment',
            title='ä¸åŒä¾†æºä¹‹æƒ…æ„Ÿåˆ†æ',
            color_discrete_map={'æ­£é¢': 'green', 'ä¸­ç«‹': 'blue', 'è² é¢': 'red'}
        )
        st.plotly_chart(fig2)

    # ----- 3. ç¹ªè£½æ™‚é–“åºåˆ—è¶¨å‹¢åœ–ï¼šä¾æ—¥æœŸçœ‹æ­£ã€ä¸­ã€è² çš„è®ŠåŒ– -----
    #st.write("DEBUG: DataFrame columns:", df.columns.tolist())
    if 'ç™¼ä½ˆæ™‚é–“' in df.columns:
        #st.write("DEBUG: Sample 'ç™¼ä½ˆæ™‚é–“' values:", df['ç™¼ä½ˆæ™‚é–“'].head().tolist())
        try:
            # ...existing code...
            df['æ—¥æœŸ'] = pd.to_datetime(df['ç™¼ä½ˆæ™‚é–“'], errors='coerce').dt.date
            df = df[~df['æ—¥æœŸ'].isna()]
            df['sentiment'] = df['sentiment'].astype(str).str.strip()
            df = df[df['sentiment'].isin(['æ­£é¢', 'ä¸­ç«‹', 'è² é¢'])]

            # å…ˆ groupby
            date_sentiment = df.groupby(['æ—¥æœŸ', 'sentiment']).size().reset_index(name='count')

            # å†é¡¯ç¤º
            #st.dataframe(df[['æ—¥æœŸ', 'sentiment']].head(20))
            #st.dataframe(df['sentiment'].value_counts())
            #st.dataframe(date_sentiment)

            #st.write("DEBUG: date_sentiment shape:", date_sentiment.shape)
            #st.write("DEBUG: date_sentiment head:", date_sentiment.head(10))

            #st.write("DEBUG: éæ¿¾å¾Œè³‡æ–™ç­†æ•¸ï¼š", len(df))
            #st.write("DEBUG: æ—¥æœŸæ¬„ NaN æ•¸é‡ï¼š", df['æ—¥æœŸ'].isna().sum())
            #st.write("DEBUG: Sample 'sentiment' values:", df['sentiment'].head(10).tolist())
            #st.write("DEBUG: sentiment dtype:", df['sentiment'].dtype)
            #st.write("DEBUG: sentiment unique:", df['sentiment'].unique())
            #st.write("DEBUG: sentiment value counts:", df['sentiment'].value_counts())

            if not date_sentiment.empty:
                fig3 = px.line(
                    date_sentiment,
                    x='æ—¥æœŸ',
                    y='count',
                    color='sentiment',
                    title='æƒ…æ„Ÿè¶¨å‹¢ï¼ˆä¾æ—¥æœŸï¼‰',
                    color_discrete_map={'æ­£é¢': 'green', 'ä¸­ç«‹': 'blue', 'è² é¢': 'red'}
                )
                st.plotly_chart(fig3)
            else:
                st.info("DEBUG: æ²’æœ‰æƒ…æ„Ÿè¶¨å‹¢è³‡æ–™å¯ä¾›ç¹ªè£½ã€‚")
            # ...existing code...
        except Exception as e:
            st.error(f"æ—¥æœŸè½‰æ›æˆ–ç¹ªåœ–æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    else:
        st.info("DEBUG: DataFrame ä¸­æ²’æœ‰ 'ç™¼ä½ˆæ™‚é–“' æ¬„ã€‚")

    # ----- 4. ç”¢ç”Ÿè©é›² -----
    # 4.1 æº–å‚™ text_corpus
    # å½±ç‰‡è©é›²
    text_corpus = ""
    if language == 'zh':
        parts = []
        if 'æ¨™é¡Œ' in df.columns:
            parts.append(" ".join(df['æ¨™é¡Œ'].dropna().astype(str).tolist()))
        if 'æè¿°' in df.columns:
            parts.append(" ".join(df['æè¿°'].dropna().astype(str).tolist()))
        if 'å…§å®¹' in df.columns:
            parts.append(" ".join(df['å…§å®¹'].dropna().astype(str).tolist()))
        # æ–°å¢ç•™è¨€æ‘˜è¦
        if 'ç•™è¨€æ‘˜è¦' in df.columns:
            parts.append(" ".join(df['ç•™è¨€æ‘˜è¦'].dropna().astype(str).tolist()))
        raw = " ".join(parts)
        chinese_text = "".join(re.findall(r'[\u4e00-\u9fff]+', raw))
        text_corpus = chinese_text
    else:  # language == 'en'
        parts = []
        if 'æè¿°' in df.columns:
            parts.append(" ".join(df['æè¿°'].dropna().astype(str).tolist()))
        if 'å…§å®¹' in df.columns:
            parts.append(" ".join(df['å…§å®¹'].dropna().astype(str).tolist()))
        # æ–°å¢ç•™è¨€æ‘˜è¦
        if 'ç•™è¨€æ‘˜è¦' in df.columns:
            parts.append(" ".join(df['ç•™è¨€æ‘˜è¦'].dropna().astype(str).tolist()))
        text_corpus = " ".join(parts)
    # å½±ç‰‡è©é›²çµæŸ.

    # æ–°èè©é›².
    text_corpus = ""
    if language == 'zh':
        # æŠŠ æ¨™é¡Œã€æè¿°ã€å…§å®¹ è£¡çš„ä¸­æ–‡éƒ½ä¸²èµ·ä¾†
        parts = []
        if 'æ¨™é¡Œ' in df.columns:
            parts.append(" ".join(df['æ¨™é¡Œ'].dropna().astype(str).tolist()))
        if 'æè¿°' in df.columns:
            parts.append(" ".join(df['æè¿°'].dropna().astype(str).tolist()))
        if 'å…§å®¹' in df.columns:
            parts.append(" ".join(df['å…§å®¹'].dropna().astype(str).tolist()))
        raw = " ".join(parts)

        # åªç•™ä¸‹æ‰€æœ‰é€£çºŒçš„æ¼¢å­—
        chinese_text = "".join(re.findall(r'[\u4e00-\u9fff]+', raw))
        text_corpus = chinese_text
    else:  # language == 'en'
        # è‹±æ–‡å°±æŠ“æè¿°+å…§å®¹ï¼Œç›´æ¥æ‹¼èµ·ä¾†
        parts = []
        if 'æè¿°' in df.columns:
            parts.append(" ".join(df['æè¿°'].dropna().astype(str).tolist()))
        if 'å…§å®¹' in df.columns:
            parts.append(" ".join(df['å…§å®¹'].dropna().astype(str).tolist()))
        text_corpus = " ".join(parts)

    # 4.2 å¦‚æœæœ‰ text_corpusï¼Œç¹ªè£½è©é›²
    if text_corpus:
        #st.write(f"DEBUG: text_corpus é•·åº¦={len(text_corpus)}")
        #st.write(f"DEBUG: å‰100å­—={text_corpus[:100]}")
        st.subheader("æ–‡å­—è©é›²å±•ç¤º")
        if language == 'zh':
            font_path = "NotoSansTC-Regular.ttf"
            tokens = " ".join(jieba.cut(text_corpus))
            wc = WordCloud(
                font_path=font_path,
                width=800,
                height=400,
                background_color='white',
                max_words=100,
                stopwords=None,
                collocations=False
            ).generate(tokens)
        else:
            # è‹±æ–‡ä¸éœ€è¦æŒ‡å®š font_path
            wc = WordCloud(
                width=800,
                height=400,
                background_color='white',
                max_words=100,
                stopwords=None,
                collocations=False
            ).generate(text_corpus)

        fig_wc, ax_wc = plt.subplots(figsize=(10, 5))
        ax_wc.imshow(wc, interpolation='bilinear')
        ax_wc.axis('off')
        st.pyplot(fig_wc)
    else:
        st.info("ç„¡å¯ç”¨æ–‡å­—ç”Ÿæˆè©é›²ã€‚")
    # æ–°èè©é›²çµæŸ.

def build_analysis_context(all_results):
    context = ""
    for source_type, df in all_results.items():
        if not df.empty:
            context += f"ã€{source_type}åˆ†ææ‘˜è¦ã€‘\n"
            context += f"ç¸½æ•¸ï¼š{len(df)}\n"
            context += f"æ­£é¢ï¼š{sum(df['sentiment']=='æ­£é¢')}\n"
            context += f"ä¸­ç«‹ï¼š{sum(df['sentiment']=='ä¸­ç«‹')}\n"
            context += f"è² é¢ï¼š{sum(df['sentiment']=='è² é¢')}\n"
            context += f"å¹³å‡æ¥µæ€§ï¼š{df['polarity'].mean():.2f}\n"
            context += f"å¹³å‡ä¸»è§€æ€§ï¼š{df['subjectivity'].mean():.2f}\n"
            context += "\n"
    return context

# é¡¯ç¤ºåˆ†æçµæœçš„å‡½å¼
def display_results(df, source_type):
    if df.empty:
        st.warning(f"æ‰¾ä¸åˆ°ä»»ä½• {source_type}")
        return
    
    # é¡¯ç¤ºåŸºæœ¬è³‡è¨Š
    st.subheader(f"{source_type} åˆ†æçµæœ")
    st.write(f"å…±åˆ†æ {len(df)} å€‹ {source_type}")
    
    # é¡¯ç¤ºæƒ…æ„Ÿåˆ†ä½ˆ
    sentiment_distribution = df['sentiment'].value_counts()
    st.write("æƒ…æ„Ÿåˆ†ä½ˆï¼š")
    st.write(sentiment_distribution)
    
    # é¡¯ç¤ºè³‡æ–™è¡¨
    st.subheader(f"{source_type} è³‡æ–™è¡¨")
    st.dataframe(df)
    
    # æƒ…æ„Ÿè¦–è¦ºåŒ–
    st.subheader(f"{source_type} æƒ…æ„Ÿè¦–è¦ºåŒ–")
    visualize_sentiment(df, language)
    
    # é¡¯ç¤ºæƒ…æ„Ÿæœ€æ¥µç«¯çš„é …ç›®
    st.subheader(f"{source_type} ä¸­æƒ…æ„Ÿæœ€æ­£å‘çš„é …ç›®")
    most_positive = df.loc[df['polarity'].idxmax()]
    st.write(f"æ¨™é¡Œï¼š{most_positive.get('æ¨™é¡Œ', 'ç„¡')}")
    st.write(f"æ¥µæ€§æŒ‡æ•¸ï¼š{most_positive['polarity']:.4f}")
    st.write(f"é€£çµï¼š{most_positive.get('é€£çµ', 'ç„¡')}")
    
    st.subheader(f"{source_type} ä¸­æƒ…æ„Ÿæœ€è² å‘çš„é …ç›®")
    most_negative = df.loc[df['polarity'].idxmin()]
    st.write(f"æ¨™é¡Œï¼š{most_negative.get('æ¨™é¡Œ', 'ç„¡')}")
    st.write(f"æ¥µæ€§æŒ‡æ•¸ï¼š{most_negative['polarity']:.4f}")
    st.write(f"é€£çµï¼š{most_negative.get('é€£çµ', 'ç„¡')}")

# ä¸»ä»‹é¢
st.title("ğŸ“° æ–°èæƒ…æ„Ÿåˆ†æAIç³»çµ±")
st.markdown("""
æœ¬ç³»çµ±å¯æœå°‹ä¸¦åˆ†æç¶²è·¯ä¸Šçš„æ–°èå…§å®¹ï¼ˆåŒ…å«æ–‡å­—èˆ‡å½±ç‰‡ï¼‰ï¼Œé¡¯ç¤ºå…¶æƒ…æ„Ÿå‚¾å‘ã€‚
""")

# --- å´é‚Šæ¬„ ---
with st.sidebar:
    st.header("API è¨­å®š")
    llm_api_key = st.text_input("LLM API é‡‘é‘°", type="password")
    news_api_key = st.text_input("NewsAPI é‡‘é‘°", type="password")
    youtube_api_key = st.text_input("YouTube API é‡‘é‘°", type="password")
    st.header("æœå°‹è¨­å®š")
    query = st.text_input("æœå°‹é—œéµå­—ï¼ˆä¸­è‹±æ–‡çš†å¯ï¼‰")
    col1, col2 = st.columns(2)
    with col1:
        days_ago = st.number_input("æœå°‹è‡³å¹¾å¤©å‰çš„æ–°è", min_value=1, max_value=30, value=7)
    with col2:
        language = st.selectbox("èªè¨€", options=['zh', 'en'], index=0)
    max_results = st.slider("æœ€å¤šé¡¯ç¤ºå½±ç‰‡æ•¸é‡", min_value=5, max_value=50, value=10)
    search_type = st.multiselect("æœå°‹é¡å‹", ['æ–‡å­—æ–°è', 'YouTube å½±ç‰‡'], default=['æ–‡å­—æ–°è'])
    analyze_button = st.button("é–‹å§‹åˆ†æ")

# --- åˆ‡æ›èªè¨€æ™‚è‡ªå‹•æ¸…ç©ºåˆ†æçµæœ ---
if "last_language" not in st.session_state:
    st.session_state["last_language"] = language
if language != st.session_state["last_language"]:
    st.session_state["all_results"] = {}
    st.session_state.messages = []
    st.session_state["last_language"] = language

# --- AI åŠ©ç†å°å°è©±æ¡† ---
with st.expander("ğŸ’¬ AI åŠ©ç† (Gemini)"):
    gemini_api_key = llm_api_key
    if "messages" not in st.session_state:
        st.session_state.messages = []
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.chat_message("user").write(msg["content"])
        else:
            st.chat_message("assistant").write(msg["content"])
    analysis_context = ""
    if "all_results" in st.session_state:
        analysis_context = build_analysis_context(st.session_state["all_results"])
    user_input = st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ...")
    if user_input and gemini_api_key:
        st.session_state.messages.append({"role": "user", "content": user_input})
        prompt = f"ä»¥ä¸‹æ˜¯åˆ†æè³‡æ–™æ‘˜è¦ï¼š\n{analysis_context}\n\nä½¿ç”¨è€…æå•ï¼š{user_input}"
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={gemini_api_key}"
        headers = {"Content-Type": "application/json"}
        payload = {"contents": [{"parts": [{"text": prompt}]}]}
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            reply = data["candidates"][0]["content"]["parts"][0]["text"]
        except Exception as e:
            reply = f"API è«‹æ±‚å¤±æ•—: {e}"
        st.session_state.messages.append({"role": "assistant", "content": reply})
        st.rerun()

# --- åˆ†ææµç¨‹ï¼ˆåªåšè³‡æ–™è™•ç†èˆ‡å­˜æª”ï¼Œä¸é¡¯ç¤ºï¼‰ ---
if analyze_button and query:
    st.session_state["all_results"] = {}
    st.session_state.messages = []
    all_results = {}
    to_date = datetime.now().strftime('%Y-%m-%d')
    from_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')

    # æ–‡å­—æ–°è
    if 'æ–‡å­—æ–°è' in search_type and news_api_key:
        news_df = get_news(query, from_date, to_date, language)
        if not news_df.empty:
            sentiment_results = []
            progress_bar = st.progress(0)
            for i, (_, row) in enumerate(news_df.iterrows()):
                text_to_analyze = f"{row['æ¨™é¡Œ']} {row['æè¿°']} {row['å…§å®¹']}"
                sentiment_data = analyze_sentiment(text_to_analyze)
                sentiment_results.append(sentiment_data)
                progress_bar.progress((i + 1) / len(news_df))
            progress_bar.empty()
            news_df['polarity'] = [r['polarity'] for r in sentiment_results]
            news_df['subjectivity'] = [r['subjectivity'] for r in sentiment_results]
            news_df['sentiment'] = [r['sentiment'] for r in sentiment_results]
            all_results['æ–‡å­—æ–°è'] = news_df

    # YouTube å½±ç‰‡
    if 'YouTube å½±ç‰‡' in search_type and youtube_api_key:
        videos_df = search_youtube_videos(query, language, max_results)
        if not videos_df.empty:
            progress_bar = st.progress(0)
            progress_text = st.empty()
            all_comments = []
            for i, (_, row) in enumerate(videos_df.iterrows()):
                progress_text.text(f"æ­£åœ¨åˆ†æç¬¬ {i+1} éƒ¨å½±ç‰‡ï¼Œå…± {len(videos_df)} éƒ¨")
                comments = get_youtube_comments(row['å½±ç‰‡ID'], youtube_api_key, max_results=100)
                for c in comments:
                    c['å½±ç‰‡æ¨™é¡Œ'] = row['æ¨™é¡Œ']
                    c['å½±ç‰‡ç™¼ä½ˆæ™‚é–“'] = row['ç™¼ä½ˆæ™‚é–“']
                all_comments.extend(comments)
                progress_bar.progress((i + 1) / len(videos_df))
            progress_bar.empty()
            progress_text.empty()
            if all_comments:
                comments_df = pd.DataFrame(all_comments)
                # å…ˆæ”¹æ¬„ä½åç¨±
                comments_df.rename(columns={'ç•™è¨€å…§å®¹': 'å…§å®¹', 'ç•™è¨€æ™‚é–“': 'ç™¼ä½ˆæ™‚é–“'}, inplace=True)
                sentiment_results = [analyze_sentiment(c) for c in comments_df['å…§å®¹']]
                comments_df['polarity'] = [r['polarity'] for r in sentiment_results]
                comments_df['subjectivity'] = [r['subjectivity'] for r in sentiment_results]
                comments_df['sentiment'] = [r['sentiment'] for r in sentiment_results]
                all_results['YouTube å½±ç‰‡'] = comments_df

    st.session_state["all_results"] = all_results

# --- é¡¯ç¤ºæµç¨‹ï¼ˆå”¯ä¸€ä¸€ä»½ï¼Œé¡¯ç¤ºåˆ†é ã€åœ–è¡¨ã€ä¸‹è¼‰ï¼‰ ---
if "all_results" in st.session_state and st.session_state["all_results"]:
    all_results = st.session_state["all_results"]
    tabs = st.tabs([t for t in all_results.keys()] + ["ç¸½çµ"])
    for idx, (source_type, df) in enumerate(all_results.items()):
        with tabs[idx]:
            display_results(df, source_type)
    with tabs[-1]:
        st.header("æƒ…æ„Ÿåˆ†æç¸½çµ")
        summary_data = []
        for source_type, df in all_results.items():
            if not df.empty:
                source_summary = {
                    'é¡å‹': source_type,
                    'ç¸½æ•¸': len(df),
                    'æ­£é¢': sum(df['sentiment'] == 'æ­£é¢'),
                    'ä¸­ç«‹': sum(df['sentiment'] == 'ä¸­ç«‹'),
                    'è² é¢': sum(df['sentiment'] == 'è² é¢'),
                    'å¹³å‡æ¥µæ€§': df['polarity'].mean(),
                    'å¹³å‡ä¸»è§€æ€§': df['subjectivity'].mean()
                }
                summary_data.append(source_summary)
        if summary_data:
            summary_df = pd.DataFrame(summary_data)
            st.write("ç¸½çµè³‡æ–™ï¼š")
            st.dataframe(summary_df)
            # ...ï¼ˆä½ çš„é•·æ¢åœ–ã€çµè«–ç­‰ç…§åŸæœ¬æ”¾é€™è£¡ï¼‰...
        else:
            st.warning("ç›®å‰æ²’æœ‰ä»»ä½•è³‡æ–™å¯ä¾›ç¸½çµåˆ†æã€‚")
    # ä¸‹è¼‰åŠŸèƒ½
    st.subheader("ä¸‹è¼‰åˆ†æçµæœ")
    for source_type, df in all_results.items():
        if not df.empty:
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label=f"ä¸‹è¼‰ {source_type} çµæœ (CSV)",
                data=csv,
                file_name=f"åˆ†æçµæœ_{source_type.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.csv",
                mime="text/csv"
            )
else:
    st.info("è«‹è¼¸å…¥æœå°‹é—œéµå­—ä¸¦é»æ“Šã€Œé–‹å§‹åˆ†æã€ä»¥å•Ÿå‹•ç³»çµ±ã€‚")
    with st.expander("ä½¿ç”¨èªªæ˜"):
        st.markdown("""
            ### ğŸ§  å¦‚ä½•ä½¿ç”¨ã€Œæ–°èæƒ…æ„Ÿåˆ†æ AI ç³»çµ±ã€

            é€™æ˜¯ä¸€å¥—æ•´åˆæ–°èã€YouTube ç•™è¨€ã€æƒ…æ„Ÿåˆ†æèˆ‡ AI åŠ©ç†çš„äº’å‹•å¼ç³»çµ±ï¼Œå¹«åŠ©ä½ å¿«é€ŸæŒæ¡ç†±é–€è©±é¡Œçš„å…¬çœ¾æƒ…ç·’å‚¾å‘ã€‚

            ---

            #### ğŸ” 1. API è¨­å®šï¼ˆå¿…å¡«æ‰èƒ½ä½¿ç”¨ï¼‰

            è«‹å…ˆæº–å‚™ä»¥ä¸‹ä¸‰çµ„é‡‘é‘°ï¼Œä¸¦è¼¸å…¥æ–¼å·¦å´é‚Šæ¬„ï¼š

            - **NewsAPI é‡‘é‘°**ï¼šç”¨ä¾†æœå°‹æ–°èå…§å®¹ï¼ˆ[è¨»å†Š NewsAPI](https://newsapi.org) ä¸¦å–å¾— API Keyï¼‰ã€‚
            - **YouTube API é‡‘é‘°**ï¼šç”¨ä¾†æœå°‹ YouTube å½±ç‰‡ä¸¦æ“·å–ç•™è¨€ï¼ˆè«‹åˆ° [Google Cloud Console](https://console.cloud.google.com/) å»ºç«‹å°ˆæ¡ˆä¸¦å•Ÿç”¨ YouTube Data APIï¼‰ã€‚
            - **LLMï¼ˆAI åŠ©ç†ï¼‰API é‡‘é‘°**ï¼šå¯é¸ã€‚æ”¯æ´ Gemini AI åŠ©ç†äº’å‹•åŠŸèƒ½ã€‚è«‹å¡«å…¥ Google Gemini çš„ API é‡‘é‘°ï¼ˆå¦‚ä½¿ç”¨ OpenAI å¯è‡ªè¡Œæ”¹ç¨‹å¼æ”¯æ´ï¼‰ã€‚

            âš ï¸ **è«‹å¦¥å–„ä¿å­˜é‡‘é‘°ï¼Œé¿å…æ´©æ¼æˆ–èª¤ç”¨ã€‚**

            ---

            #### ğŸ” 2. æœå°‹è¨­å®šï¼ˆåœ¨å·¦é‚Šæ¬„èª¿æ•´ï¼‰

            - **æœå°‹é—œéµå­—**ï¼šè¼¸å…¥æƒ³è¦åˆ†æçš„ä¸»é¡Œè©ï¼ˆå¯ä½¿ç”¨ä¸­æ–‡æˆ–è‹±æ–‡ï¼‰ã€‚
            - **æœå°‹å¤©æ•¸**ï¼šç³»çµ±æœƒå¾ä»Šå¤©å¾€å‰è¨ˆç®—ï¼Œä¾‹å¦‚è¼¸å…¥ã€Œ7ã€å³ä»£è¡¨æœå°‹æœ€è¿‘ 7 å¤©çš„æ–°è/å½±ç‰‡ã€‚
            - **èªè¨€é¸æ“‡**ï¼š
            - `zh`ï¼šä»¥ä¸­æ–‡æœå°‹æ–°èèˆ‡å½±ç‰‡ï¼Œä¸¦é€²è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æã€‚
            - `en`ï¼šä»¥è‹±æ–‡æœå°‹æ–°èèˆ‡å½±ç‰‡ï¼Œä¸¦é€²è¡Œæƒ…æ„Ÿåˆ†æã€‚
            - **æœ€å¤šé¡¯ç¤ºå½±ç‰‡æ•¸é‡**ï¼šå¾ 5 åˆ° 50 éƒ¨å½±ç‰‡ä¹‹é–“èª¿æ•´ã€‚
            - **æœå°‹é¡å‹ï¼ˆå¯å¤šé¸ï¼‰**ï¼š
            - `æ–‡å­—æ–°è`ï¼šå¾æ–°èç¶²ç«™æŠ“å–æ–‡ç« è³‡æ–™ã€‚
            - `YouTube å½±ç‰‡`ï¼šæŠ“å–å½±ç‰‡ç•™è¨€é€²è¡Œæƒ…æ„Ÿåˆ†æã€‚

            è¨­å®šå®Œæˆå¾Œï¼Œè«‹é»æ“Š **ã€Œé–‹å§‹åˆ†æã€** æŒ‰éˆ•ï¼Œç³»çµ±å°‡é–‹å§‹é‹ä½œã€‚

            ---

            #### ğŸ¤– 3. AI åŠ©ç†åŠŸèƒ½ï¼ˆGeminiï¼‰

            ä½ å¯ä»¥æ‰“é–‹ä¸‹æ–¹çš„ **ã€ŒğŸ’¬ AI åŠ©ç† (Gemini)ã€** å°è©±æ¡†ï¼š

            - å•Ÿç”¨æ¢ä»¶ï¼šå·²è¼¸å…¥ LLM API é‡‘é‘°ã€‚
            - åŠŸèƒ½ï¼šAI åŠ©ç†æœƒæ ¹æ“šä½ è¼¸å…¥çš„å•é¡Œèˆ‡åˆ†æçµæœï¼Œä½¿ç”¨ Google Gemini å›è¦†å»ºè­°èˆ‡è§£é‡‹ã€‚
            - ç¯„ä¾‹å•é¡Œï¼š
            - ã€Œé€™æ¬¡çš„é—œéµå­—æƒ…æ„Ÿåå‘å¦‚ä½•ï¼Ÿã€
            - ã€Œå“ªå€‹ä¾†æºçš„è² é¢æƒ…ç·’æœ€å¤šï¼Ÿã€
            - ã€Œè«‹å¹«æˆ‘æ‘˜è¦é€™æ¬¡åˆ†æé‡é»ã€‚ã€

            ---

            #### ğŸ“Š 4. æŸ¥çœ‹åˆ†æçµæœ

            æ¯å€‹è³‡æ–™ä¾†æºï¼ˆæ–‡å­—æ–°èæˆ–å½±ç‰‡ï¼‰éƒ½æœƒæœ‰ç¨ç«‹åˆ†é ï¼Œé¡¯ç¤ºï¼š

            - **æƒ…æ„Ÿçµ±è¨ˆ**ï¼š
            - æ­£é¢ / ä¸­ç«‹ / è² é¢ æ•¸é‡
            - æƒ…æ„Ÿæ¥µæ€§æŒ‡æ•¸èˆ‡ä¸»è§€æ€§å¹³å‡å€¼
            - **å¯è¦–åŒ–åœ–è¡¨**ï¼š
            - åœ“é¤…åœ–ï¼šæƒ…æ„Ÿæ¯”ä¾‹åˆ†ä½ˆ
            - é•·æ¢åœ–ï¼šä¸åŒä¾†æºçš„æƒ…ç·’æ¯”å°
            - æŠ˜ç·šåœ–ï¼šéš¨æ™‚é–“çš„æƒ…ç·’è®ŠåŒ–è¶¨å‹¢
            - **æ–‡å­—é›²ï¼ˆè©é›²åœ–ï¼‰**ï¼š
            - ç›´è§€å±•ç¤ºé‡è¦é—œéµå­—
            - **æ¥µç«¯æƒ…ç·’æ¨£æœ¬**ï¼š
            - æ­£é¢èˆ‡è² é¢æƒ…ç·’æœ€å¼·çš„æ–‡ç« æˆ–ç•™è¨€å…§å®¹èˆ‡é€£çµ

            ---

            #### ğŸ“¥ 5. è³‡æ–™ç¸½çµèˆ‡ä¸‹è¼‰

            - ç³»çµ±æœƒè‡ªå‹•æ•´åˆç¸½çµè¡¨æ ¼ï¼ˆå„ä¾†æºçš„æƒ…ç·’çµ±è¨ˆï¼‰ï¼Œä¸¦å¯ä¾› CSV ä¸‹è¼‰ã€‚
            - æ¯å€‹è³‡æ–™ä¾†æºï¼ˆæ–°èæˆ–å½±ç‰‡ï¼‰éƒ½å¯åˆ†åˆ¥åŒ¯å‡ºåˆ†æçµæœï¼ˆå«æ¨™é¡Œã€ä¾†æºã€æƒ…æ„Ÿæ¨™è¨»ç­‰ï¼‰ã€‚

            ---

            #### ğŸ§ª 6. ç¯„ä¾‹é—œéµå­—å»ºè­°ï¼ˆä¸­æ–‡ / è‹±æ–‡çš†å¯ï¼‰

            ä»¥ä¸‹ç‚ºä½ å¯ä»¥è©¦ç”¨çš„ç†±é–€ä¸»é¡Œé—œéµå­—ï¼š

            - æ”¿æ²» (Politics)  
            - ç¶“æ¿Ÿ (Economy)  
            - AI æŠ€è¡“ (Artificial Intelligence)  
            - ç’°ä¿ (Environment)  
            - å¥åº· (Health)  
            - æ•™è‚² (Education)  
            - é›»å‹•è»Š (Electric Vehicle)  
            - ChatGPTã€Geminiã€OpenAI  

            ---

            #### ğŸ’¡ æé†’äº‹é …

            - è‹¥ç³»çµ±é¡¯ç¤ºã€Œç„¡è³‡æ–™ã€ï¼Œå¯èƒ½æ˜¯ï¼š
            - é—œéµå­—éå†·é–€ï¼Œæ²’æœ‰æ–°èæˆ–å½±ç‰‡ã€‚
            - API é‡‘é‘°æœªå¡«æˆ–æ¬¡æ•¸é™åˆ¶å·²é”ä¸Šé™ã€‚
            - å»ºè­°é¸æ“‡è¼ƒç†±é–€æˆ–æ™‚äº‹æ€§çš„ä¸»é¡Œè©ä½œç‚ºé—œéµå­—ã€‚

            """)
        
# é é¢åº•éƒ¨
st.markdown("---")
st.markdown("Â© 2025 æ–°èæƒ…æ„Ÿåˆ†æ AI ç³»çµ± â€” ä½¿ç”¨ Streamlitã€NewsAPIã€YouTube APIã€Gemini AI é–‹ç™¼ã€‚")
